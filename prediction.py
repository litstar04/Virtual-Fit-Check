# -*- coding: utf-8 -*-
"""Copy of Collectivism.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iXVOrg0euOT5eR1liY7dbelFViLVQjNM

#Main Preprocessing

##Fetching & Looking at Data
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import euclidean_distances
import os
# def load_clothing_data():
#     path = "img_details.csv"  # Replace with the actual path to the CSV file
#     return pd.read_csv(path)

clothing_num_data=pd.read_csv('Males_img_details.csv')
# clothing_num_data['id_column']=clothing_num_data.index+1
# # Creating index values from 1 till row count
# import numpy as np
# clothing_num_data['id_column'] = np.arange(1, clothing_num_data.shape[0] + 1)
# for a in clothing_num_data.index:
#   clothing_num_data['id_column']=a+1
#   print(a)

# clothing_num_data

# import pandas as pd

# # Add a new column at the start that acts as an index/id
# clothing_num_data.insert(0, 'ID', range(1, len(clothing_num_data) + 1))

# # print(clothing_num_data)

metrics=clothing_num_data.drop(["image_name"], axis=1)
clothes=clothing_num_data.drop(list(metrics.columns),axis=1)
# print(metrics)
# print(clothes)











# from zlib import crc32

# def test_set_check(identifier, test_ratio):
#     return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

# def split_train_test_by_id(data, test_ratio, id_column):
#     ids = data[id_column]
#     in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
#     return data.loc[~in_test_set], data.loc[in_test_set]

# train_set, test_set = split_train_test_by_id(clothing_num_data, 0.2, "ID")

# for set in(train_set, test_set):
#     set.drop("image_name", axis=1, inplace=True)
# corr_matrix=clothing_num_data.corr()

# train_set, test_set

from sklearn.impute import SimpleImputer
imputer1 = SimpleImputer(strategy="median")
# imputer2 = SimpleImputer(strategy="median")
imputer1.fit(metrics)
# imputer2.fit(clothes)
X=imputer1.transform(metrics)
# clothes = imputer2.transform(clothes)
metrics_f=pd.DataFrame(
    X,
    columns=metrics.columns,
    index=metrics.index
)

# metrics_f['leg_length']

# metrics_f.values[:,0]



from sklearn.base import BaseEstimator, TransformerMixin

# column index
leg_length,torso_length,shoulder_width,hip_width = 0,1,2, 3

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, val=True): # no *args or **kargs
        self.val = val
    def fit(self, X, y=None):
        return self  # nothing else to do
    def transform(self, X):
        # Height(cm)	Waist	Chest	Gender	Complexion
        # HWC=X[:, Height]  + X[:, Waist] + X[:, Chest]
        leg_torso= X[:,leg_length] / X[:,torso_length]
        leg_shoulder=X[:,leg_length] / X[:,shoulder_width]
        leg_hip=X[:,leg_length] / X[:,hip_width]
        torso_shoulder=X[:,torso_length] / X[:,shoulder_width]
        torso_hip=X[:,torso_length] / X[:,hip_width]
        shoulder_hip=X[:,shoulder_width] / X[:,hip_width]
        # rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        # population_per_household = X[:, population_ix] / X[:, households_ix]
        new_features = np.c_[
            leg_torso,
            leg_shoulder,
            leg_hip,
            torso_shoulder,
            torso_hip,
            shoulder_hip
        ]
        return np.c_[new_features] if self.val else X


        # if self.val:
        #     return np.c_[X, leg_torso]
        # else:
        #     return self

attr_adder = CombinedAttributesAdder()
metrics_extra_attribs = attr_adder.transform(metrics_f.values)
metrics_real_attribs = pd.DataFrame(
    metrics_extra_attribs,
    columns=["leg_torso","leg_shoulder","leg_hip","torso_shoulder","torso_hip","shoulder_hip"])
# metrics_extra_attribs
# metrics_real_attribs
# print(type(metrics_f))

# from sklearn.base import BaseEstimator, TransformerMixin

# # column index
# leg_length,torso_length,shoulder_width,hip_width = 0,1,2, 3

# class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
#     def __init__(self, val=True): # no *args or **kargs
#         self.val = val
#     def fit(self, X, y=None):
#         return self  # nothing else to do
#     def transform(self, X):
#         # Height(cm)	Waist	Chest	Gender	Complexion
#         # HWC=X[:, Height]  + X[:, Waist] + X[:, Chest]
#         leg_torso= X[:,leg_length] / X[:,torso_length]
#         leg_shoulder=X[:,leg_length] / X[:,shoulder_width]
#         leg_hip=X[:,leg_length] / X[:,hip_width]
#         torso_shoulder=X[:,torso_length] / X[:,shoulder_width]
#         torso_hip=X[:,torso_length] / X[:,hip_width]
#         shoulder_hip=X[:,shoulder_width] / X[:,hip_width]
#         # rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
#         # population_per_household = X[:, population_ix] / X[:, households_ix]
#         new_features = np.c_[
#             leg_torso,
#             leg_shoulder,
#             leg_hip,
#             torso_shoulder,
#             torso_hip,
#             shoulder_hip
#         ]
#         return np.c_[X,new_features] if self.val else X


#         # if self.val:
#         #     return np.c_[X, leg_torso]
#         # else:
#         #     return self

# attr_adder = CombinedAttributesAdder()
# metrics_extra_attribs = attr_adder.transform(metrics_f.values)
# # metrics_extra_attribs = pd.DataFrame(
# #     metrics_extra_attribs,
# #     columns=["leg_torso","leg_shoulder","leg_hip","torso_shoulder","torso_hip","shoulder_hip"])
# # metrics_extra_attribs.head()
# metrics_extra_attribs









#path - file location

def final_func(path):
  #imports dataset
  clothing_num_data=pd.read_csv(path)
  # clothing_num_data.insert(0, 'ID', range(1, len(clothing_num_data) + 1))
  metrics=clothing_num_data.drop(["image_name"], axis=1)
  clothes=clothing_num_data.drop(list(metrics.columns),axis=1)
  # print(metrics)
  # print(clothes)



  #Simple Imputer for median calculations
  imputer1 = SimpleImputer(strategy="median")
  imputer1.fit(metrics)
  X=imputer1.transform(metrics)
  metrics_f=pd.DataFrame(
      X,
      columns=metrics.columns,
      index=metrics.index
  )
  # print(metrics_f,'\n')



  # ratio calculated
  attr_adder = CombinedAttributesAdder()
  metrics_extra_attribs = attr_adder.transform(metrics_f.values)
  metrics_real_attribs = pd.DataFrame(
      metrics_extra_attribs,
      columns=["leg_torso","leg_shoulder","leg_hip","torso_shoulder","torso_hip","shoulder_hip"])
  # metrics_extra_attribs.head()
  # print(metrics_real_attribs,'\n')


  scaler = StandardScaler()
  final_metrics = scaler.fit_transform(metrics_real_attribs)
  # print("final_metrics",final_metrics)
  dataset_f = pd.DataFrame(
    final_metrics,
    columns=["leg_torso","leg_shoulder","leg_hip","torso_shoulder","torso_hip","shoulder_hip"])
  # print(dataset_f)
  return dataset_f,clothes,scaler

dataset_f,clothes,scaler=final_func('Males_img_details.csv')
# upar wala function double hoga ek baar male ke liye ek baar female ke liye

# print(dataset_f,clothes)
# print(type(clothes))

# xyz,abcd=final_func('img_details.csv')
# print(xyz,abcd)

scaler



"""#Get The Preprocessed Data"""

#Final Dataset de rakha hoga so remove this later
# final_dataset=pd.read_csv('img_details.csv',index_col='image_name')
# final_dataset
final_dataset=dataset_f
# final_dataset = models ki measurements
# clothes = clothes ki measurements
# final

dataset_f.iloc[158]

print(clothes.iloc[78].values)

final_dataset.drop('group_no.', axis=1, errors='ignore', inplace=True) #Changed 




"""#KMeans Models"""

import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans



# def optimise_k_means(data, max_k):
#   means = []
#   inertias = []

#   for k in range(1, max_k+1):
#     kmeans = KMeans(n_clusters=k)
#     kmeans.fit(data)

#     means.append(k)
#     inertias.append(kmeans.inertia_)

#   # Generate the elbow plot
#   fig =plt.subplots(figsize=(10, 5))
#   plt.plot(means, inertias, 'o-')
#   plt.xlabel('Number of Clusters')
#   plt.ylabel('Inertia')
#   plt.grid(True)
#   plt.show()

# optimise_k_means(final_dataset.drop('group_no.',axis=1), 5)





kmeans = KMeans(n_clusters=4)
kmeans.fit(final_dataset)
# yeh bhi do banenge

final_dataset['group_no.']=kmeans.labels_

# final_dataset.values

# pd.DataFrame(kmeans.cluster_centers_)
#

# kmeans.cluster_centers_

# prompt: drop the 'group_no.' column of the final_dataset dataframe

# final_dataset = final_dataset.drop('group_no.', axis=1)

# final_dataset

# final_dataset.to_csv('males.csv')





"""#Plotting The Clustering Graph"""

# plt.scatter(x=final_dataset['leg_shoulder'], y=final_dataset['leg_hip'], c=final_dataset['group_no.'])
# plt.xlabel('leg_shoulder')
# plt.ylabel('leg_hip')
# plt.show()





"""#User Metrics Standardization"""

# final_dataset

import pandas as pd
from sklearn.impute import SimpleImputer

# Assuming CombinedAttributesAdder is already defined elsewhere

def user_data_prep(path, scaler):
    # Import dataset
    clothing_num_data = pd.read_csv(path)
    
    # Check if 'image_name' column exists and drop it if present
    if 'image_name' in clothing_num_data.columns:
        metrics = clothing_num_data.drop(["image_name"], axis=1)
    else:
        # If 'image_name' doesn't exist, just use the data as is
        metrics = clothing_num_data.copy()

    # Extract clothing-related data (non-metric data)
    clothes = clothing_num_data.drop(list(metrics.columns), axis=1)
    
    # Impute missing values using median strategy
    imputer1 = SimpleImputer(strategy="median")
    imputer1.fit(metrics)
    X = imputer1.transform(metrics)
    
    # Convert the imputed data to a DataFrame
    metrics_f = pd.DataFrame(X, columns=metrics.columns, index=metrics.index)

    # Calculate additional ratios (assuming CombinedAttributesAdder is implemented)
    attr_adder = CombinedAttributesAdder()
    metrics_extra_attribs = attr_adder.transform(metrics_f.values)
    
    # Create a new DataFrame with additional calculated attributes
    metrics_real_attribs = pd.DataFrame(
        metrics_extra_attribs,
        columns=["leg_torso", "leg_shoulder", "leg_hip", "torso_shoulder", "torso_hip", "shoulder_hip"]
    )

    # Standardize the metrics using the provided scaler
    final_metrics = scaler.transform(metrics_real_attribs)
    
    # Convert final metrics to a DataFrame
    dataset_f = pd.DataFrame(
        final_metrics,
        columns=["leg_torso", "leg_shoulder", "leg_hip", "torso_shoulder", "torso_hip", "shoulder_hip"]
    )
    
    return dataset_f, clothes

user_metrics,userimg = user_data_prep('measurements.csv',scaler)
# import from previous files - scaler m aur f ke liye, final_dataset m & f, kmeans maur f

user_metrics

pd.read_csv('measurements.csv')





"""#User Dataset and Final Dataset Prepared and clustered till here

#From here creating the dataset for the similarity calculations
"""

final_dataset_copy = final_dataset

# final_dataset
# final_dataset_copy

# def k_means_algo(data,k):
#   kmeans = KMeans(n_clusters=k)
#   kmeans.fit(data)
#   return kmeans

# def clustered_set(model_dataset,testing_set,user_dataset,k):
#   kmeans=k_means_algo(model_dataset,k)
#   num=kmeans.predict(user_dataset)[0]
#   temp_set=model_dataset[model_dataset['group_no.']==num]
#   testing_set=pd.concat([testing_set,temp_set])
#   model_dataset=model_dataset.drop(temp_set.index)
#   print(testing_set,'\n')
#   print(model_dataset,'\n')
#   return model_dataset,testing_set

# kmeans.cluster_centers_

# user_metrics.values



centroids = kmeans.cluster_centers_

# New data point (same number of features as the original dataset)
new_data = user_metrics.values

# Calculate the Euclidean distance from the new data point to each centroid
distances = [np.linalg.norm(new_data - centroid) for centroid in centroids]
distances
# # Get the indices of the top 3 closest centroids based on the smallest distances
top_3_clusters = np.argsort(distances)[:3]
top_3_clusters
# # Print the top 3 closest clusters
# print("Top 3 closest clusters to the new data point:")
arr=[]
for cluster_idx in top_3_clusters:
    arr.append(cluster_idx)
    # print(f"Cluster {cluster_idx}: Distance = {distances[cluster_idx]:.2f}")

# max=arr[np.argmax(arr)]
# arr

# final_dataset

# arr

# np.argmax(arr)

# centroids





def clustered_set(model_dataset,testing_set,num):
  temp_set=model_dataset[model_dataset['group_no.']==num]
  testing_set=pd.concat([testing_set,temp_set])
  model_dataset=model_dataset.drop(temp_set.index)
  # print(testing_set,'\n')
  # print(model_dataset,'\n')
  return model_dataset,testing_set

testing_set=pd.DataFrame()
for num in arr:
  final_dataset_copy,testing_set=clustered_set(final_dataset_copy,testing_set,num)

# final_dataset

# testing_set

"""#Till here the Testing_set for created and now similarity can be applied

##Euclid Recommendation
"""

# testing_set, user_metrics

# # import numpy as np
# # import pandas as pd
# # from sklearn.preprocessing import StandardScaler
# from sklearn.metrics.pairwise import euclidean_distances

# euclid_dists = euclidean_distances(testing_set.drop('group_no.',axis=1), user_metrics).flatten()

# euclid_dists

# euclid_dists_points = np.argsort(euclid_dists)

# euclid_dists_points[:5]

# for i in euclid_dists_points[:3]:
#   print(euclid_dists[i])



# testing_set.iloc[euclid_dists_points[:5]].index

# euclidean_indices=np.array(testing_set.iloc[euclid_dists_points[:5]].index)

# if os.path.isfile('recommendations.csv'):
#   clothes.iloc[euclidean_indices].to_csv('recommendations.csv')
# else:
#   clothes.iloc[euclidean_indices].to_csv('recommendations.csv', mode='a', header=False)



def euclid_func(testing_set,user_metrics):
  euclid_dists = euclidean_distances(testing_set.drop('group_no.',axis=1), user_metrics).flatten()
  euclid_dists_points = np.argsort(euclid_dists)
  testing_set.iloc[euclid_dists_points[:5]].index
  euclidean_indices=np.array(testing_set.iloc[euclid_dists_points[:5]].index)
  if os.path.isfile('euclid_recommends.csv')==False:
    clothes.iloc[euclidean_indices].to_csv('euclid_recommends.csv')
  else:
    clothes.iloc[euclidean_indices].to_csv('euclid_recommends.csv', mode='a', header=False)
  return euclid_dists

euclid_dists=euclid_func(testing_set,user_metrics)

# euclid_dists



"""##Cosine Similarity (incase dataset is too large and the backend part takes too long to run)"""

# testing_set, user_metrics



# from sklearn.metrics.pairwise import cosine_similarity

# # Compute cosine similarity
# cos_similarities = cosine_similarity(testing_set.drop('group_no.',axis=1), user_metrics).flatten()

# # Sort in descending order (higher similarity is better)
# nearest_indices_cos = np.argsort(-cos_similarities)[:4]
# # closest_clothes_cos = labels[nearest_indices_cos]

# # Display results
# print("\nClosest matches using Cosine Similarity:")
# for idx in nearest_indices_cos:
#     print(f"Similarity: {cos_similarities[idx]:.4f}")



# """##Code After Applying PCA"""

# copy_for_PCA,user_metrics_copy=testing_set.drop('group_no.',axis=1),user_metrics

# copy_for_PCA,user_metrics_copy

# from sklearn.decomposition import PCA

# # Reduce dimensions to 2 for faster calculations
# pca = PCA(n_components=3)

# features_reduced = pca.fit_transform(copy_for_PCA)
# user_ratios_reduced = pca.transform(user_metrics_copy)

# # Compute distances in reduced space
# distances_reduced = euclidean_distances(features_reduced, user_ratios_reduced).flatten()

# distances_reduced

# distances_reduced-euclid_dists

# r=(distances_reduced-euclid_dists)/euclid_dists

# r

